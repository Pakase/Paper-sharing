{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "896b246c",
   "metadata": {},
   "source": [
    "### ICLR 2023 (Notable)\n",
    "\n",
    "##### 1.Rectified Flow: A Marginal Preserving Approach to Optimal Transport\n",
    "https://arxiv.org/pdf/2209.14577.pdf  \n",
    "###### Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow  (ICLR 2023)\n",
    "https://arxiv.org/abs/2209.03003  \n",
    "这两篇论文总的来说，是在讲如何构建一个Sraight的ODEs(Neural ordinary differential equations)，可以用于构建Generative Models的损失函数。不同于Diffusion，Diffusion Models是SDEs(stochastic ordinary differential equations)，用噪声提供stochastic，用时间插值模拟生成。Diffusion需要多个时间步采样模拟，但Rectified Flow可以实现one-step生成，在较低时间步(N=1)时就能有较好的生成效果。\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "\n",
    "##### 2.*StyleAlign: Analysis and Applications of Aligned StyleGAN Models (ICLR 2022)\n",
    "https://openreview.net/pdf?id=Qg2vi4ZbHM9  \n",
    "**The paper shows that when a model is obtained (fine-tuned) from another, then the corresponding hidden semantic spaces are aligned.** The paper uses this property to show that without any additional architecture or training, the models can perform diverse tasks such as image translation and morphing. The paper also demonstrates that zero-shot tasks can be performed by learning in the parent domain and transferring to the child domain.\n",
    "\n",
    "--------------------------------\n",
    "\n",
    "##### 3.Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions (ICLR 2023)  \n",
    "https://openreview.net/pdf?id=zyLVMgsZ0U_  \n",
    "Keywords: diffusion models, score-based generative models, sampling, score estimation, Langevin, stochastic differential equations  \n",
    "TL;DR: We prove that given an L2-accurate score estimate, diffusion models can sample from (essentially) any data distribution, even if it is highly non-log-concave and/or supported on a low dimensional manifold.  \n",
    "Abstract: We provide theoretical convergence guarantees for score-based generative models (SGMs) such as denoising diffusion probabilistic models (DDPMs), which constitute the backbone of large-scale real-world generative models such as DALL\n",
    "E 2. Our main result is that, assuming accurate score estimates, such SGMs can efficiently sample from essentially any realistic data distribution. In contrast to prior works, our results (1) hold for an L^2 accurate score estimate (rather than \n",
    "L^infinite accurate); (2) do not require restrictive functional inequality conditions that preclude substantial non-log-concavity; (3) scale polynomially in all relevant problem parameters; and (4) match state-of-the-art complexity guarantees for discretization of the Langevin diffusion, provided that the score error is sufficiently small. We view this as strong theoretical justification for the empirical success of SGMs. We also examine SGMs based on the critically damped Langevin diffusion (CLD). Contrary to conventional wisdom, we provide evidence that the use of the CLD does *not* reduce the complexity of SGMs.\n",
    "\n",
    "----------------------------\n",
    "\n",
    "##### 4.gDDIM: Generalized denoising diffusion implicit models(ICLR2023)  \n",
    "https://openreview.net/pdf?id=1hKE9qjvz-  \n",
    "Keywords: Fast sampling, diffusion model   \n",
    "Abstract: Our goal is to extend the denoising diffusion implicit model (DDIM) to general diffusion models(DMs) besides isotropic diffusions. Instead of constructing a non-Markov noising process as in the original DDIM, we examine the mechanism of DDIM from a numerical perspective.  We discover that the DDIM can be obtained by using some specific approximations of the score when solving the corresponding stochastic differential equation. We present an interpretation of the accelerating effects of DDIM that also explains the advantages of a deterministic sampling scheme over the stochastic one for fast sampling. Building on this insight, we extend DDIM to general DMs, coined generalized DDIM (gDDIM), with a small but delicate modification in parameterizing the score network. We validate gDDIM in two non-isotropic DMs: Blurring diffusion model (BDM) and Critically-damped Langevin diffusion model (CLD). We observe more than 20 times acceleration in BDM. In the CLD, a diffusion model by augmenting the diffusion process with velocity, our algorithm achieves an FID score of 2.26, on CIFAR10, with only 50 number of score function evaluations~(NFEs) and an FID score of 2.86 with only 27 NFEs.\n",
    "\n",
    "----------------\n",
    "\n",
    "##### 5.Learning multi-scale local conditional probability models of images(ICLR 2023)\n",
    "https://openreview.net/pdf?id=VZX2I_VVJKH  \n",
    "Keywords: Image priors, Markov wavelet conditional models, multi-scale score-based image synthesis, denoising, super-resolution  \n",
    "Abstract: Deep neural networks can learn powerful prior probability models for images, as evidenced by the high-quality generations obtained with recent score-based diffusion methods. But the means by which these networks capture complex global statistical structure, apparently without suffering from the curse of dimensionality, remain a mystery. To study this, we incorporate diffusion methods into a multi-scale decomposition, reducing dimensionality by assuming a stationary local Markov model for wavelet coefficients conditioned on coarser-scale coefficients. We instantiate this model using convolutional neural networks (CNNs) with local receptive fields, which enforce both the stationarity and Markov properties. Global structures are captured using a CNN with receptive fields covering the entire (but small) low-pass image. We test this model on a dataset of face images, which are highly non-stationary and contain large-scale geometric structures.\n",
    "Remarkably, denoising, super-resolution, and image synthesis results all demonstrate that these structures can be captured with significantly smaller conditioning neighborhoods than required by a Markov model implemented in the pixel domain. Our results show that score estimation for large complex images can be reduced to low-dimensional Markov conditional models across scales,  alleviating the curse of dimensionality. \n",
    "\n",
    "---------\n",
    "\n",
    "##### 6.Few-shot Cross-domain Image Generation via Inference-time Latent-code Learning(ICLR 2023)\n",
    "https://openreview.net/pdf?id=sCYXJr3QJM8  \n",
    "Keywords: generative domain adaptation, generative adversarial network  \n",
    "Abstract: In this work, our objective is to adapt a Deep generative model trained on a large-scale source dataset to multiple target domains with scarce data. Specifically, we focus on adapting a pre-trained Generative Adversarial Network (GAN) to a target domain without re-training the generator. Our method draws the motivation from the fact that out-of-distribution samples can be 'embedded' onto the latent space of a pre-trained source-GAN. We propose to train a small latent-generation network during the inference stage, each time a  batch of target samples is to be generated. These target latent codes are fed to the source-generator to obtain  novel target samples. Despite using the same small set of target samples and the source generator, multiple independent training episodes of the latent-generation network results in the diversity of the generated target samples. Our method, albeit simple, can be used to generate data from multiple target distributions using a generator trained on a single source distribution. We demonstrate the efficacy of our surprisingly simple method in generating multiple target datasets with only a single source generator and a few target samples.  \n",
    "\n",
    "-------\n",
    "\n",
    "##### 7.Learning Diffusion Bridges on Constrained Domains(ICLR 2023)  \n",
    "https://openreview.net/pdf?id=WH1yCa0TbB  \n",
    "Abstract: Diffusion models have achieved promising results on generative learning recently. However, because diffusion processes are most naturally applied  on the unconstrained Euclidean space R^d, key challenges arise for developing diffusion based models for learning data on constrained and structured domains. We present a simple and unified framework to achieve this that can be easily adopted to various types of domains, including product spaces of any type (be it bounded/unbounded, continuous/discrete, categorical/ordinal, or  their mix). In our model, the diffusion process is driven by a drift force that is a sum of two terms: one singular force designed by Doob's h-transform that ensures all outcomes of the process to belong to the desirable domain, and one non-singular neural force field that is trained to make sure the outcome follows the data distribution statistically. Experiments show that our methods perform superbly on generating tabular data, images, semantic segments and 3D point clouds.   \n",
    "\n",
    "-----------\n",
    "\n",
    "##### 8.DIFFormer: Scalable (Graph) Transformers Induced by Energy Constrained Diffusion (ICLR 2023)\n",
    "https://openreview.net/pdf?id=j6zUzrapY3L  \n",
    "Keywords: structured representation learning, diffusion model, optimization-induced model, node prediction  \n",
    "Abstract: Real-world data generation often involves complex inter-dependencies among instances, violating the IID-data hypothesis of standard learning paradigms and posing a challenge for uncovering the geometric structures for learning desired instance representations. To this end, we introduce an energy constrained diffusion model which encodes a batch of instances from a dataset into evolutionary states that progressively incorporate other instances' information by their interactions. The diffusion process is constrained by descent criteria w.r.t. a principled energy function that characterizes the global consistency of instance representations over latent structures. We provide rigorous theory that implies closed-form optimal estimates for the pairwise diffusion strength among arbitrary instance pairs, which gives rise to a new class of neural encoders, dubbed as DIFFormer (diffusion-based Transformers), with two instantiations: a simple version with linear complexity for prohibitive instance numbers, and an advanced version for learning complex structures. Experiments highlight the wide applicability of our model as a general-purpose encoder backbone with superior performance in various tasks, such as node classification on large graphs, semi-supervised image/text classification, and spatial-temporal dynamics prediction. The codes are available at https://github.com/qitianwu/DIFFormer.  \n",
    "\n",
    "---------\n",
    "\n",
    "##### 9.Continual Unsupervised Disentangling of Self-Organizing Representations(ICLR 2023)\n",
    "https://openreview.net/pdf?id=ih0uFRFhaZZ  \n",
    "Keywords: **continual disentanglment**, generative model, VAE, SOM  \n",
    "TL;DR: We proposed a novel generative model describing a topologically-connected mixture of spike-and-slab distributions in the latent space for continual unsupervised learning and disentangling representations.  \n",
    "Abstract: Limited progress has been made in continual unsupervised learning of representations, especially in reusing, expanding, and continually disentangling learned semantic factors across data environments. We argue that this is because existing approaches treat continually-arrived data independently, without considering how they are related based on the underlying semantic factors. We address this by a new generative model describing a topologically-connected mixture of spike-and-slab distributions in the latent space, learned end-to-end in a continual fashion via principled variational inference. The learned mixture is able to automatically discover the active semantic factors underlying each data environment and to accumulate their relational structure based on that. This distilled knowledge of different data environments can further be used for generative replay and guiding continual disentangling of new semantic factors. We tested the presented method on a split version of 3DShapes to provide the first quantitative disentanglement evaluation of continually learned representations, and further demonstrated its ability to continually disentangle new representations in benchmark datasets.  \n",
    "\n",
    "-----------\n",
    "\n",
    "##### 10.DiffEdit: Diffusion-based semantic image editing with mask guidance(ICLR 2023)\n",
    "https://openreview.net/pdf?id=3lge0p5o-M-  \n",
    "Keywords: computer vision, image editing, diffusion models  \n",
    "Abstract: Image generation has recently seen tremendous advances, with diffusion models allowing to synthesize convincing images for a large variety of text prompts. In this article, we propose DiffEdit, a method to take advantage of text-conditioned diffusion models for the task of semantic image editing, where the goal is to edit an image based on a text query. Semantic image editing is an extension of image generation, with the additional constraint that the generated image should be as similar as possible to a given input image. \n",
    "Current editing methods based on diffusion models usually require to provide a mask, making the task much easier by treating it as a conditional inpainting task. In contrast, our main contribution is able to automatically generate a mask highlighting regions of the input image that need to be edited, by contrasting predictions of a diffusion model conditioned on different text prompts. Moreover, we rely on latent inference to preserve content in those regions of interest and show excellent synergies with mask-based diffusion. \n",
    "DiffEdit achieves state-of-the-art editing performance on ImageNet. In addition, we evaluate semantic image editing in more challenging settings, using images from the COCO dataset as well as text-based generated images.  \n",
    "\n",
    "-------------\n",
    "\n",
    "##### 11.Prompt-to-Prompt Image Editing with Cross-Attention Control(ICLR 2023)\n",
    "https://openreview.net/pdf?id=_CDixzkzeyb  \n",
    "Keywords: Image generation, Image editing, Diffusion models, Attention layer, Computer vision, Machine learning  \n",
    "Abstract: Recent large-scale text-driven synthesis diffusion models have attracted much attention thanks to their remarkable capabilities of generating highly diverse images that follow given text prompts. Therefore, it is only natural to build upon these synthesis models to provide text-driven image editing capabilities. However, Editing is challenging for these generative models, since an innate property of an editing technique is to preserve some content from the original image, while in the text-based models, even a small modification of the text prompt often leads to a completely different outcome. State-of-the-art methods mitigate this by requiring the users to provide a spatial mask to localize the edit, hence, ignoring the original structure and content within the masked region. In this paper, we pursue an intuitive prompt-to-prompt editing framework, where the edits are controlled by text only. We analyze a text-conditioned model in depth and observe that the cross-attention layers are the key to controlling the relation between the spatial layout of the image to each word in the prompt. With this observation, we propose to control the attention maps along the diffusion process. Our approach enables us to monitor the synthesis process by editing the textual prompt only, paving the way to a myriad of caption-based editing applications such as localized editing by replacing a word, global editing by adding a specification, and even controlling the extent to which a word is reflected in the image. We present our results over diverse images and prompts with different text-to-image models, demonstrating high-quality synthesis and fidelity to the edited prompts.  \n",
    "\n",
    "---------------\n",
    "\n",
    "##### 12.Multifactor Sequential Disentanglement via Structured Koopman Autoencoders (ICLR 2023)\n",
    "https://openreview.net/pdf?id=6fuPIe9tbnC  \n",
    "Keywords: Koopman methods, Sequential Disentanglement  \n",
    "TL;DR: A new method for learning multifactor disentangled representations of sequential data  \n",
    "Abstract: Disentangling complex data to its latent factors of variation is a fundamental task in representation learning. Existing work on sequential disentanglement mostly provides two factor representations, i.e., it separates the data to time-varying and time-invariant factors. In contrast, we consider multifactor disentanglement in which multiple (more than two) semantic disentangled components are generated. Key to our approach is a strong inductive bias where we assume that the underlying dynamics can be represented linearly in the latent space. Under this assumption, it becomes natural to exploit the recently introduced Koopman autoencoder models. However, disentangled representations are not guaranteed in Koopman approaches, and thus we propose a novel spectral loss term which leads to structured Koopman matrices and disentanglement. Overall, we propose a simple and easy to code new deep model that is fully unsupervised and it supports multifactor disentanglement. We showcase new disentangling abilities such as swapping of individual static factors between characters, and an incremental swap of disentangled factors from the source to the target. Moreover, we evaluate our method extensively on two factor standard benchmark tasks where we significantly improve over competing unsupervised approaches, and we perform competitively in comparison to weakly- and self-supervised state-of-the-art approaches. The code is available at https://github.com/azencot-group/SKD.  \n",
    "\n",
    "----------------\n",
    "\n",
    "##### 13.An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion(ICLR 2023)\n",
    "https://openreview.net/pdf?id=NAQvF08TcyG  \n",
    "Keywords: Personalized generation, text-to-image, inversion  \n",
    "TL;DR: We present the task of personalized text-to-image generation, and introduce an inversion-based method that allows us to synthesize novel scenes of user-provided visual concepts, guided by natural language instructions.  \n",
    "Abstract: Text-to-image models offer unprecedented freedom to guide creation through natural language. Yet, it is unclear how such freedom can be exercised to generate images of specific unique concepts, modify their appearance, or compose them in new roles and novel scenes.  \n",
    "In other words, we ask: how can we use language-guided models to turn *our* cat into a painting, or imagine a new product based on *our* favorite toy?   \n",
    "Here we present a simple approach that allows such creative freedom. Using only 3-5 images of a user-provided concept, like an object or a style, we learn to represent it through new ''words\" in the embedding space of a frozen text-to-image model.\n",
    "These ''words\" can be composed into natural language sentences, guiding *personalized* creation in an intuitive way.  \n",
    "Notably, we find evidence that a *single* word embedding is sufficient for capturing unique and varied concepts.   \n",
    "We compare our approach to a wide range of baselines, and demonstrate that it can more faithfully portray the concepts across a range of applications and tasks. Our code, data and new words will be available. \n",
    "\n",
    "--------------\n",
    "\n",
    "##### 14.Zero-Shot Image Restoration Using Denoising Diffusion Null-Space Model(ICLR 2023)\n",
    "https://openreview.net/forum?id=mRieQgMtNTQ  \n",
    "Keywords: Zero-Shot, Inverse Problems, Super-Resolution, Diffusion Models, Range-Null Space, Image Restoration, Colorization, Compressed Sensing, Inpainting, Deblur, Old Photo Restoration, Blind Restoration  \n",
    "Abstract: Most existing Image Restoration (IR) models are task-specific, which can not be generalized to different degradation operators. In this work, we propose the **Denoising Diffusion Null-Space Model (DDNM)**, a novel zero-shot framework for arbitrary linear IR problems, including but not limited to image super-resolution, colorization, inpainting, compressed sensing, and deblurring. DDNM only needs a pre-trained off-the-shelf diffusion model as the generative prior, without any extra training or network modifications. By refining only the null-space contents during the reverse diffusion process, we can yield diverse results satisfying both data consistency and realness. We further propose an enhanced and robust version, dubbed DDNM+, to support noisy restoration and improve restoration quality for hard tasks. Our experiments on several IR tasks reveal that DDNM outperforms other state-of-the-art zero-shot IR methods. We also demonstrate that DDNM+ can solve complex real-world applications, e.g., old photo restoration.   \n",
    "\n",
    "--------------\n",
    "\n",
    "##### 15.Diffusion Models Already Have A Semantic Latent Space (ICLR 2023)\n",
    "https://openreview.net/pdf?id=pd1P2eUBVfq  \n",
    "Keywords: diffusion models, semantic image editing  \n",
    "Abstract: Diffusion models achieve outstanding generative performance in various domains. Despite their great success, they lack semantic latent space which is essential for controlling the generative process. To address the problem, we propose asymmetric reverse process (Asyrp) which discovers the semantic latent space in frozen pretrained diffusion models. Our semantic latent space, named h-space, has nice properties for accommodating semantic image manipulation: homogeneity, linearity, robustness, and consistency across timesteps. In addition, we measure editing strength and quality deficiency of a generative process at timesteps to provide a principled design of the process for versatility and quality improvements. Our method is applicable to various architectures (DDPM++, iDDPM, and ADM) and datasets (CelebA-HQ, AFHQ-dog, LSUN-church, LSUN-bedroom, and METFACES).  \n",
    "\n",
    "-----------\n",
    "\n",
    "##### 16.Deterministic training of generative autoencoders using invertible layers(ICLR 2023)\n",
    "https://openreview.net/pdf?id=g8wBdhnstYz  \n",
    "Abstract: In this work, we provide a deterministic alternative to the stochastic variational training of generative autoencoders. We refer to these new generative autoencoders as AutoEncoders within Flows (AEF), since the encoder and decoder are defined as affine layers of an overall invertible architecture. This results in a deterministic encoding of the data, as opposed to the stochastic encoding of VAEs. The paper introduces two related families of AEFs. The first family relies on a partition of the ambient space and is trained by exact maximum-likelihood. The second family exploits a deterministic expansion of the ambient space and is trained by maximizing the log-probability in this extended space. This latter case leaves complete freedom in the choice of encoder, decoder and prior architectures, making it a drop-in replacement for the training of existing VAEs and VAE-style models. We show that these AEFs can have strikingly higher performance than architecturally identical VAEs in terms of log-likelihood and sample quality, especially for low dimensional latent spaces. Importantly, we show that AEF samples are substantially sharper than VAE samples.   \n",
    "\n",
    "-------\n",
    "\n",
    "##### 17.Diffusion Posterior Sampling for General Noisy Inverse Problems (ICLR 2023)\n",
    "https://openreview.net/pdf?id=OnD9zGAGT0k  \n",
    "Keywords: Diffusion model, Inverse problem, Posterior sampling  \n",
    "TL;DR: We propose a diffusion model-based general inverse problem solver that scales to nonlinear problems and different noise statistics.  \n",
    "Abstract: Diffusion models have been recently studied as powerful generative inverse problem solvers, owing to their high quality reconstructions and the ease of combining existing iterative solvers. However, most works focus on solving simple linear inverse problems in noiseless settings, which significantly under-represents the complexity of real-world problems. In this work, we extend diffusion solvers to efficiently handle general noisy (non)linear inverse problems via the Laplace approximation of the posterior sampling. Interestingly, the resulting posterior sampling scheme is a blended version of diffusion sampling with the manifold constrained gradient without a strict measurement consistency projection step, yielding a more desirable generative path in noisy settings compared to the previous studies. Our method demonstrates that diffusion models can incorporate various measurement noise statistics such as Gaussian and Poisson, and also efficiently handle noisy nonlinear inverse problems such as Fourier phase retrieval and non-uniform deblurring.  \n",
    "\n",
    "-------\n",
    "##### 18.SketchKnitter: Vectorized Sketch Generation with Diffusion Models (ICLR 2023)\n",
    "https://openreview.net/pdf?id=4eJ43EN2g6l  \n",
    "**亮点：用Diffusion生成矢量图**  \n",
    "Abstract: We show vectorized sketch generation can be identified as a reversal of the stroke deformation process. This relationship was established by means of a diffusion model that learns data distributions over the stroke-point locations and pen states of real human sketches. Given randomly scattered stroke-points, sketch generation becomes a process of deformation-based denoising, where the generator rectifies positions of stroke points at each timestep to converge at a recognizable sketch. A key innovation was to embed recognizability into the reverse time diffusion process. It was observed that the estimated noise during the reversal process is strongly correlated with sketch classification accuracy. An auxiliary recurrent neural network (RNN) was consequently used to quantify recognizability during data sampling. It follows that, based on the recognizability scores, a sampling shortcut function can also be devised that renders better quality sketches with fewer sampling steps. Finally it is shown that the model can be easily extended to a conditional generation framework, where given incomplete and unfaithful sketches, it yields one that is more visually appealing and with higher recognizability.  \n",
    "\n",
    "----\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af85b0d",
   "metadata": {},
   "source": [
    "**********************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fed5ed1",
   "metadata": {},
   "source": [
    "### ICLR 2023 (Poster)\n",
    "\n",
    "##### 1.Diffusion-GAN: Training GANs with Diffusion\n",
    "https://openreview.net/pdf?id=HZf7UbpWHuA  \n",
    "Keywords: deep generative models, diffusion models, data-efficient stable GAN training, adaptive data augmentation  \n",
    "Abstract: Generative adversarial networks (GANs) are challenging to train stably, and a promising remedy of injecting instance noise into the discriminator input has not been very effective in practice.  In this paper, we propose Diffusion-GAN, a novel GAN framework that leverages a forward diffusion chain to generate Gaussian-mixture distributed instance noise. Diffusion-GAN consists of three components, including an adaptive diffusion process, a diffusion timestep-dependent discriminator, and a generator. Both the observed and generated data are diffused by the adaptive diffusion process via different noise-to-data ratios at each timestep. The timestep-dependent discriminator learns to distinguish the diffused real data from the diffused generated data at each diffusion timestep. The generator learns from the discriminator's feedback by backpropagating through the forward diffusion chain, whose length is adaptively adjusted to balance the noise and data levels. We theoretically show that the discriminator's timestep-dependent strategy gives consistent and helpful guidance to the generator, enabling it to match the true data distribution. We demonstrate the advantages of Diffusion-GAN over strong GAN baselines on various datasets, showing that it can produce more realistic images with higher stability and data efficiency than state-of-the-art GANs.  \n",
    "\n",
    "--------\n",
    "\n",
    "##### 2.Forward Super-Resolution: How Can GANs Learn Hierarchical Generative Models for Real-World Distributions\n",
    "https://openreview.net/pdf?id=7h5KSs2PCRi  \n",
    "TL;DR: We provide a theory to study how generative adversarial networks (GANs) can efficiently learn certain hierarchically generated distributions that are close to the distribution of images in practice.  \n",
    "Abstract: Generative adversarial networks (GANs) are among the most successful models for learning high-complexity, real-world distributions. However, in theory, due to the highly non-convex, non-concave landscape of the minmax training objective, GAN remains one of the least understood deep learning models. In this work, we formally study how GANs can efficiently learn certain hierarchically generated distributions that are close to the distribution of real-life images. We prove that when a distribution has a structure that we refer to as \\emph{forward super-resolution}, then simply training generative adversarial networks using stochastic gradient descent ascent (SGDA) can learn this distribution efficiently, both in sample and time complexities.  \n",
    "We also provide empirical evidence that our assumption ''forward super-resolution'' is very natural in practice, and the underlying learning mechanisms that we study in this paper (to allow us efficiently train GAN via GDA in theory) simulates the actual learning process of GANs on real-world problems.  \n",
    "\n",
    "-----\n",
    "\n",
    "##### 3.Where to Diffuse, How to Diffuse, and How to Get Back: Automated Learning for Multivariate Diffusions\n",
    "https://openreview.net/pdf?id=osei3IzUia  \n",
    "Keywords: Diffusion models, score based generative model, generative models, variational inference    \n",
    "Abstract: Diffusion-based generative models (DBGMs) perturb data to a target noise distribution and reverse this process to generate samples.  The choice of noising process, or inference diffusion process, affects both likelihoods and sample quality. For example, extending the inference process with auxiliary variables leads to improved sample quality. While there are many such multivariate diffusions to explore, each new one requires significant model-specific analysis, hindering rapid prototyping and evaluation. In this work, we study Multivariate Diffusion Models (MDMs). For any number of auxiliary variables, we provide a recipe for maximizing a lower-bound on the MDMs likelihood without requiring any model-specific analysis. We then demonstrate how to parameterize the diffusion for a specified target noise distribution; these two points together enable optimizing the inference diffusion process. Optimizing the diffusion expands easy experimentation from just a few well-known processes to an automatic search over all linear diffusions. To demonstrate these ideas, we introduce two new specific diffusions as well as learn a diffusion process on the MNIST, CIFAR10, and ImageNet32 datasets. We show learned MDMs match or surpass bits-per-dims (BPDs) relative to fixed choices of diffusions for a given dataset and model architecture.  \n",
    "\n",
    "-----\n",
    "\n",
    "##### 4.Score-based Continuous-time Discrete Diffusion Models\n",
    "https://openreview.net/pdf?id=BYWWwSY2G5s  \n",
    "Keywords: discrete space diffusion, discrete score matching, continuous-time diffusion  \n",
    "Abstract: Score-based modeling through stochastic differential equations (SDEs) has provided a new perspective on diffusion models, and demonstrated superior performance on continuous data. However, the gradient of the log-likelihood function, \\ie, the score function, is not properly defined for discrete spaces. This makes it non-trivial to adapt SDE with score functions to categorical data. In this paper, we extend diffusion models to discrete variables by introducing a stochastic jump process where the reverse process denoises via a continuous-time Markov chain. This formulation admits an analytical simulation during backward sampling. To learn the reverse process, we extend score matching to general categorical data, and show that an unbiased estimator can be obtained via simple matching of the conditional marginal distributions. We demonstrate the effectiveness of the proposed method on a set of synthetic and real-world music and image benchmarks.  \n",
    "\n",
    "-----\n",
    "\n",
    "##### 5.Denoising Diffusion Samplers\n",
    "https://openreview.net/pdf?id=8pvnfTAbu1f  \n",
    "Keywords: diffusion models, importance sampling, monte carlo, variational inference  \n",
    "TL;DR: How to use denoising diffusion models ideas to sample unnormalized target densities and estimate their normalizing constants  \n",
    "Abstract: Denoising diffusion models are a popular class of generative models providing state-of-the-art results in many domains. One adds gradually noise to data using a diffusion to transform the data distribution into a Gaussian distribution. Samples from the generative model are then obtained by simulating an approximation of the time-reversal of this diffusion initialized by Gaussian samples. Practically, the intractable score terms appearing in the time-reversed process are approximated using score matching techniques. We explore here a similar idea to sample approximately from unnormalized probability density functions and estimate their normalizing constants. We consider a process where the target density diffuses towards a Gaussian. Denoising Diffusion Samplers (DDS) are obtained by approximating the corresponding time-reversal.  While score matching is not applicable in this context, we can leverage many of the ideas introduced in generative modeling for Monte Carlo sampling. Existing theoretical results from denoising diffusion models also provide theoretical guarantees for DDS. We discuss the connections between DDS, optimal control and Schr\\\"odinger bridges and finally demonstrate DDS experimentally on a variety of challenging sampling tasks.  \n",
    "\n",
    "-----------\n",
    "\n",
    "##### 6.Fast Sampling of Diffusion Models with Exponential Integrator\n",
    "https://openreview.net/pdf?id=Loek7hfb46P  \n",
    "Keywords: Fast diffusion model, generative model  \n",
    "TL;DR: Training-free acceleration for diffusion model, 4.17 FID with 10 NFEs on CIFAR10\n",
    "Abstract: The past few years have witnessed the great success of Diffusion models(DMs) in generating high-fidelity samples in generative modeling tasks. A major limitation of the DM is its notoriously slow sampling procedure which normally requires hundreds to thousands of time discretization steps of the learned diffusion process to reach the desired accuracy. Our goal is to develop a fast sampling method for DMs with a much less number of steps while retaining high sample quality. To this end, we systematically analyze the sampling procedure in DMs and identify key factors that affect the sample quality, among which the method of discretization is most crucial. By carefully examining the learned diffusion process, we propose Diffusion Exponential Integrator Sampler(DEIS). It is based on the Exponential Integrator designed for discretizing ordinary differential equations (ODEs) and leverages a semilinear structure of the learned diffusion process to reduce the discretization error. The proposed method can be applied to any DMs and can generate high-fidelity samples in as few as 10 steps. Moreover, by directly using pre-trained DMs, we achieve state-of-art sampling performance when the number of score function evaluation(NFE) is limited,  e.g., 4.17 FID with 10 NFEs, 2.86 FID with only 20 NFEs on CIFAR10.  \n",
    "\n",
    "---------\n",
    "\n",
    "##### 7.Understanding DDPM Latent Codes Through Optimal Transport \n",
    "https://openreview.net/pdf?id=6PIrhAx1j4i  \n",
    "Keywords: diffusion models, ddpm, optimal transport, theory  \n",
    "Abstract: Diffusion models have recently outperformed alternative approaches to model the distribution of natural images. Such diffusion models allow for deterministic sampling via the probability flow ODE, giving rise to a latent space and an encoder map. While having important practical applications, such as the estimation of the likelihood, the theoretical properties of this map are not yet fully understood. In the present work, we partially address this question for the popular case of the VP-SDE (DDPM) approach. We show that, perhaps surprisingly, the DDPM encoder map coincides with the optimal transport map for common distributions; we support this claim by extensive numerical experiments using advanced tensor train solver for multidimensional Fokker-Planck equation. We provide additional theoretical evidence for the case of multivariate normal distributions.  \n",
    "\n",
    "---------\n",
    "\n",
    "##### 8.Blurring Diffusion Models \n",
    "https://openreview.net/pdf?id=OjDkC57x5sz  \n",
    "Keywords: blurring, diffusion, generative model  \n",
    "TL;DR: We show that blurring can equivalently be defined through a Gaussian diffusion process with non-isotropic noise, bridging the gap between inverse heat dissipation and denoising diffusion  \n",
    "Abstract: Recently, Rissanen et al., (2022) have presented a new type of diffusion process for generative modeling based on heat dissipation, or blurring, as an alternative to isotropic Gaussian diffusion. Here, we show that blurring can equivalently be defined through a Gaussian diffusion process with non-isotropic noise. In making this connection, we bridge the gap between inverse heat dissipation and denoising diffusion, and we shed light on the inductive bias that results from this modeling choice. Finally, we propose a generalized class of diffusion models that offers the best of both standard Gaussian denoising diffusion and inverse heat dissipation, which we call Blurring Diffusion Models.   \n",
    "\n",
    "--------\n",
    "\n",
    "##### 9.Accelerating Guided Diffusion Sampling with Splitting Numerical Methods\n",
    "https://openreview.net/pdf?id=F0KTk2plQzO  \n",
    "Keywords: Splitting Numerical Methods, Guided Diffusion Models  \n",
    "TL;DR: We accelerate guided diffusion sampling using splitting numerical methods.  \n",
    "Abstract: Guided diffusion is a technique for conditioning the output of a diffusion model at sampling time without retraining the network for each specific task. However, one drawback of diffusion models, whether they are guided or unguided, is their slow sampling process.   \n",
    "Recent techniques can accelerate unguided sampling by applying high-order numerical methods to the sampling process when viewed as differential equations. On the contrary, we discover that the same techniques do not work for guided sampling, and little has been explored about its acceleration. This paper explores the culprit of this problem and provides a solution based on operator splitting methods, motivated by our key finding that classical high-order numerical methods are unsuitable for the conditional function. Our proposed method can re-utilize the high-order methods for guided sampling and can generate images with the same quality as a 250-step DDIM baseline using 32-58% less sampling time on ImageNet256.   \n",
    "We also demonstrate usage on a wide variety of conditional generation tasks, such as text-to-image generation, colorization, inpainting, and super-resolution.  \n",
    "\n",
    "---------\n",
    "\n",
    "##### 10.Markup-to-Image Diffusion Models with Scheduled Sampling \n",
    "https://openreview.net/pdf?id=81VJDmOE2ol  \n",
    "Abstract: Building on recent advances in image generation, we present a fully data-driven approach to rendering markup into images. The approach is based on diffusion models, which parameterize the distribution of data using a sequence of denoising operations on top of a Gaussian noise distribution. We view the diffusion denoising process a sequential decision making process, and show that it exhibits compounding errors similar to exposure bias issues in imitation learning problems. To mitigate these issues, we adapt the scheduled sampling algorithm to diffusion training. We conduct experiments on four markup datasets: formulas (LaTeX), table layouts (HTML), sheet music (LilyPond), and molecular images (SMILES). These experiments each verify the effectiveness of diffusion and the use of scheduled sampling to fix generation issues. These results also show that the markup-to-image task presents a useful controlled compositional setting for diagnosing and analyzing generative image models.  \n",
    "\n",
    "---------\n",
    "\n",
    "##### 11.Unified Discrete Diffusion for Simultaneous Vision-Language Generation\n",
    "https://openreview.net/pdf?id=8JqINxA-2a  \n",
    "Keywords: Multi-modal, Image generation, Image Caption.  \n",
    "Abstract: The recently developed discrete diffusion model performs extraordinarily well in generation tasks, especially in the text-to-image task, showing great potential for modeling multimodal signals. In this paper, we leverage these properties and present a unified multimodal generation model, which can perform text-based, image-based, and even vision-language simultaneous generation using a single model. Specifically, we unify the discrete diffusion process for multimodal signals by proposing a unified Markov transition matrix and a unified objective. Moreover, we design a multimodal mutual attention module to highlight the inter-modal linkages, which is vital for multimodal generation. Extensive experiments indicate that our proposed method can perform comparably to the state-of-the-art solutions in various generation tasks.  \n",
    "\n",
    "----------\n",
    "\n",
    "##### 12.Truncated Diffusion Probabilistic Models and Diffusion-based Adversarial Auto-Encoders\n",
    "https://openreview.net/pdf?id=HDxgaKk956l  \n",
    "Keywords: Diffusion model, adversarial autoencoder, implicit prior  \n",
    "Abstract: Employing a forward diffusion chain to gradually map the data to a  noise distribution, diffusion-based generative models learn how to generate the data by inferring a reverse diffusion chain. However, this approach is slow and costly because it needs many forward and reverse steps. We propose a faster and cheaper approach that adds noise not until the data become pure random noise, but until they reach a hidden noisy data distribution that we can confidently learn. Then, we use fewer reverse steps to generate data by starting from this hidden distribution that is made similar to the noisy data. We reveal that the proposed model can be cast as an adversarial auto-encoder empowered by both the diffusion process and a learnable implicit prior. Experimental results show even with a significantly smaller number of reverse diffusion steps, the proposed truncated diffusion probabilistic models can provide consistent improvements over the non-truncated ones in terms of performance in both unconditional and text-guided image generations.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b785f83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
